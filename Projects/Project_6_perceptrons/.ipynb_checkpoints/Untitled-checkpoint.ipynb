{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建感知器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    '''\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, weights = np.array( [1] ), threshold = 0):\n",
    "        '''\n",
    "        Initialize weights and threshold based on input arguments. \n",
    "        Note that no type-checkin is being performed here for simplicity.\n",
    "        '''\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def activate(self, inputs):\n",
    "        '''\n",
    "        Takes in @param inputs, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on \n",
    "        perceptron weights and threshold.\n",
    "        '''\n",
    "        # calculate the strength with which the perceptron fires\n",
    "        outputs = np.dot(inputs, self.weights)\n",
    "        # return 0 or 1 based on the threshold\n",
    "        result = np.where(outputs > self.threshold, 1, 0)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    '''\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing shold show up in the output if all the assertions pass.\n",
    "    '''\n",
    "    p1 = Perceptron(np.array([1, 2]), 0.)\n",
    "    assert p1.activate(np.array([1, -1])) == 0\n",
    "    assert p1.activate(np.array([-1, 1])) == 1\n",
    "    assert p1.activate(np.array([2, -1])) == 0\n",
    "    print('Yes. End Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes. End Test\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main advantage of having a threshold be set to a perceptron is being able to control when a percenptron should fire and when it shouldn't.   \n",
    "### This gives us control on the sensitivity of out neurons thereby helping us influence the desired output.\n",
    "设置一个阈值的主要优点是能够控制感知器何时应该触发，何时不应该触发。这使我们能够控制神经元的灵敏度，从而帮助我们影响预期的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题：\n",
    "### 1. What do you think the advantage of a perceptron is, compared with simply returning the dot product without a threshold?\n",
    "    相比较于直接简单的输出点乘以后的值（没有阈值限制），感知机的优势是什么？\n",
    "\n",
    "答案：    \n",
    "Combining dot products with thresholds makes possible classification (more or less than the certain value) and regression (how close targets to the needed points).\n",
    "将点积与阈值组合在一起，可以进行分类(或多或少地超过某个值)和回归(接近所需点的距离)。\n",
    "\n",
    "### 2. 我们希望建立一个感知机，那么在建立模型的过程中，我们需要修改的是以下哪些值？\n",
    "- 输出方程 output function    \n",
    "- 输入数据 input data  \n",
    "- 阈值 threshold \n",
    "- 权重 weights\n",
    "- 输入方程 output function\n",
    "\n",
    "答案：  \n",
    "    阈值threshold、权重weights\n",
    "\n",
    "### 3. 如果我们不设定阈值，我们就只需计算一个线性方程。请问我们能和计算线性回归一样计算权重吗？\n",
    "- 可以完美地计算\n",
    "- 会算出一个结果，但是不是我们想得到的结果\n",
    "- 在此情况下无法得出结果\n",
    "- 离散的问题需要新的解决方案\n",
    "- 我们应该使用微积分来计算\n",
    "\n",
    "答案：  \n",
    "线性方程$y = w_1 x_1 + w_2 x_2 + ... + w_n x_n + w_0 $，根据模型，我们需要求出需要的损失函数。  \n",
    "一般线性回归我们用**均方误差**作为损失函数。   \n",
    "损失函数的代数法表示为：\n",
    "$$J(\\theta_0, \\theta_1, \\theta_2, ... , \\theta_n) = \\sum_{i=0}^{m} (h_\\theta(x_0, x_1, ... , x_))$$\n",
    "对于线性回归的损失函数，常用的方法来求损失函数最小化时的参数$\\theta$：\n",
    "- 梯度下降法\n",
    "- 最小二乘法\n",
    "\n",
    "而感知机的训练方式有两种：\n",
    "- 感知机规则 $y- \\hat y$\n",
    "- 梯度下降法 $y - activate function $\n",
    "\n",
    "网上没有找到标准答案，但是对应的直觉来看，答案应该是第二个：会算出一个结果，但是不是我们想得到的结果。\n",
    "\n",
    "<p><a href = 'https://www.cnblogs.com/muzixi/p/6642203.html'>线性回归，感知机，逻辑回归（GD，SGD）</a></p>\n",
    "<p><a href = 'https://www.jianshu.com/p/0fdb1e7653dd'>从线性回归、感知机到神经网络</a></p>\n",
    "\n",
    "### 4. 人工神经网络是由感知机单元构成的，人工神经网络的输入应该是什么格式的呢？\n",
    "- 带有标签的数值型矩阵\n",
    "- 有向图\n",
    "- 无标签的数值型矩阵\n",
    "- 带有标签的集合\n",
    "- 每行带有标签的数值型矩阵\n",
    "\n",
    "答案：  \n",
    "    每行带有标签的数值型矩阵\n",
    "    \n",
    "### 5. 我们能从神经网络的输出中得到什么信息？\n",
    "- 一个有向图（神经网络本身）\n",
    "- 一个标量\n",
    "- 用向量表示的分类信息\n",
    "- 每个输入向量都对应一个输出向量\n",
    "\n",
    "答案： \n",
    "一个有向图（神经网络本身）；   \n",
    "一个标量；   \n",
    "用向量表示的分类信息；   \n",
    "每个输入向量都对应一个输出向量。   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感知机更新规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights.astype(float)\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\"\n",
    "               \n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values,self.weights)\n",
    "        \n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        return int(strength > self.threshold)\n",
    "\n",
    "\n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to the perceptron training\n",
    "        rule using these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "        # for each data point...\n",
    "        for data_point in range(len(values)):\n",
    "            \n",
    "            # obtain the neuron's prediction for that point\n",
    "            predict = self.activate( values[data_point] )\n",
    "\n",
    "            # update self.weights based on prediction accuracy, learning\n",
    "            # rate and input value\n",
    "            error = train[data_point] - predict\n",
    "            weight_update = eta * error * values[data_point]\n",
    "            self.weights +=weight_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    '''\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing shold show up in the output if all the assertions pass.\n",
    "    '''\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-6):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "    \n",
    "    p1 = Perceptron(np.array([1,1,1]),0)\n",
    "    p1.update(np.array([[2,0,-3]]), np.array([1]))\n",
    "    assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7]))\n",
    "\n",
    "    p2 = Perceptron(np.array([1,2,3]),0)\n",
    "    p2.update(np.array([[3,2,1],[4,0,-1]]),np.array([0,0]))\n",
    "    assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9]))\n",
    "\n",
    "    p3 = Perceptron(np.array([3,0,2]),0)\n",
    "    p3.update(np.array([[2,-2,4],[-1,-3,2],[0,2,1]]),np.array([0,1,0]))\n",
    "    assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))\n",
    "    \n",
    "    print('END TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END TEST\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 一般情况下，我们会把一个结构化的神经网络写成如下形式：\n",
    "[[node,node,node], # 输入层 [node,node], # 隐藏层 [node]] # 输出层\n",
    "\n",
    "给定隐藏层为 [1,1,-5] 和 [3,-4,2]，输出层为 [2,-1]，如果神经网络的输入为 [1,2,3]，输出会是什么？\n",
    "\n",
    "答案：   \n",
    "![title](../../Notes/images/perceptron_answer.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quiz: Layered Network Example\n",
    "2*(1 * 1+1 * 2+(-5 * 3))-1*(1 * 3 - 2 * 4 + 3 * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 我们希望通过增加神经网络的层数来来表示更多的结果，对于单纯判断加权和的线性节点是做不到的。\n",
    "给定以下神经网络（结构为2-3-1），请写出与下列神经网络会产生同样结果的方程式的系数。\n",
    "\n",
    "神经网络的系数矩阵为：\n",
    "\n",
    "[[input,input], [[3,2],[-1,4],[3,-5]], [1,2,-1]]\n",
    "\n",
    "答案：  \n",
    "w = (-2, 15)\n",
    "![title](../../Notes/images/nn_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建XOR网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    '''\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, weights = np.array( [1] ), threshold = 0):\n",
    "        '''\n",
    "        Initialize weights and threshold based on input arguments. \n",
    "        Note that no type-checking is being performed here for simplicity.\n",
    "        '''\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def activate(self, values):\n",
    "        '''\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on \n",
    "        perceptron weights and threshold.\n",
    "        '''\n",
    "        strength = np.dot(values, self.weights)\n",
    "        return int(strength > self.threshold)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Set up the perceptron network\n",
    "Network = [\n",
    "    # input layer, declare input layer perceptrons here\n",
    "    [Perceptron( [1, 0], 0), Perceptron( [1, 1], 1), Perceptron( [0, 1], 0) ],\n",
    "    # output layer, declare output layer perceptron here\n",
    "    [Perceptron( [1, -2, 1], 0)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Define a procedure to compute the output of the network, given inputs \n",
    "def EvalNetwork(inputValues, Network):\n",
    "    '''\n",
    "    Takes in @param inputValues, a list of input values, and @param Network that specifies \n",
    "    a perceptron network. \n",
    "    @return the output of the Network for the given set of inputs.\n",
    "    '''\n",
    "    OutputValue = Network[1][0].activate( [h.activate( inputValues) for h in Network[0]])\n",
    "    # print('OutputValue',OutputValue)\n",
    "    # Be sure your output value is a single number\n",
    "    return OutputValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    '''\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    '''\n",
    "    print('0 XOR 0 = 0?:', EvalNetwork(np.array( [0, 0] ), Network))\n",
    "    print('0 XOR 1 = 1?:', EvalNetwork(np.array( [0, 1] ), Network))\n",
    "    print('1 XOR 0 = 1?:', EvalNetwork(np.array( [1, 0] ), Network))\n",
    "    print('1 XOR 1 = 0?:', EvalNetwork(np.array( [1, 1] ), Network))\n",
    "    print(\"END TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 XOR 0 = 0?: 0\n",
      "0 XOR 1 = 1?: 1\n",
      "1 XOR 0 = 1?: 1\n",
      "1 XOR 1 = 0?: 0\n",
      "END TEST\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = np.array([1,1])\n",
    "p1 = Perceptron([1,0], 0)\n",
    "p1.activate(values)\n",
    "\n",
    "p2 = Perceptron([1,1], 1)\n",
    "p2.activate(values)\n",
    "\n",
    "p3 = Perceptron([0,1], 0)\n",
    "p3.activate(values)\n",
    "\n",
    "pp = Perceptron([1, -2, 1], 0)\n",
    "pp.activate(np.array([1,2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../../Notes/images/NN_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 人工神经网络的一个问题是他只能输出离散值，这就使得他不能有效的处理回归问题，并且处理负责问题的时候需要更多的单元。\n",
    "例如： 给定一个结构为 [2,2,1]（输入层两个单元，隐藏层两个单元，输出层一个单元）的神经网络，最多可以预测几种房屋的价格？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../../Notes/images/NN_03.png)\n",
    "\n",
    "根据图上的结构可以看出最多有4种不同的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 要解决神经网络只有几个离散输出的问题，我们需要增加一个转换函数。\n",
    "\n",
    "下面从测试几个不同的函数开始，在数字上不断接近其导数，以应用梯度下降规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 激活函数沙盒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(strength):\n",
    "    '''\n",
    "    Try out different functions here. \n",
    "    Input strength will be a number, with another number as output.\n",
    "    '''\n",
    "    return np.power(strength, 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_derivative(activate, strength):\n",
    "    # numerically approximate\n",
    "    return (activate( strength + 1e-5) - activate( strength - 1e-5)) // 2e-5  # 2e-5 为0.00002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 我们已经决定使用一个连续(避免离散问题)并且非线性(允许我们表示非线性)的方程，以下哪个方程满足我们的需求？\n",
    "- Sine\n",
    "- Arctangent\n",
    "- Natural logarithm\n",
    "- Cube root\n",
    "- Logistic function\n",
    "\n",
    "答案：   \n",
    "Logistic function，其实就是阶跃函数和sigmoid函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 单个感知机和一个 Sigmoid 单元在二分类问题上有什么区别？\n",
    "- 没有区别\n",
    "- 后者给出了更多的信息，但是两者的结果会相同\n",
    "- 有时候会给出不同的结果\n",
    "- 总是给出不同的结果\n",
    "- 会在特定的情况下给出不同的结果\n",
    "\n",
    "答案：    \n",
    "后者给出了更多的信息，但是两者的结果会相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. 我们需要像训练感知机一样来训练 Sigmoid 单元。该怎么定义更新规则呢？\n",
    "- 随机\n",
    "- 凭直觉\n",
    "- 运用领域相关知识\n",
    "- 运用微积分\n",
    "- 运用三角学方法\n",
    "\n",
    "答案：   \n",
    "运算微积分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. 运用微积分，梯度下降算法可以给我们提供一个求极值的方法。但是也会产生很多问题，你认为会产生下列哪些问题？\n",
    "- 局部的极值\n",
    "- 运行太耗时\n",
    "- 会产生无限次循环\n",
    "- 无法收敛\n",
    "\n",
    "答案：   \n",
    "以上四个都有可能产生。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    '''\n",
    "    This class models an artificial neuron with sigmoid activation function. \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, weights = np.array( [1] )):\n",
    "        '''\n",
    "        Initialize weights based on input arguments. \n",
    "        Note that no type-checking is being performed here for simplicity of code.\n",
    "        '''\n",
    "        self.weights = weights\n",
    "        \n",
    "        # Note: You do not need to worry about these attributes for this programming quiz, \n",
    "        # but these will be useful for if you want to create a network out of these sigmoid units!\n",
    "        self.last_input = 0 # strength of last input\n",
    "        self.delta = 0  # error signal\n",
    "        \n",
    "    def activate(self, values):\n",
    "        '''\n",
    "        Takes in @param values, a list of numbers equal to length of weights. \n",
    "        @return the output of a sigmoid unit with given inputs based on unit weights.\n",
    "        '''\n",
    "        \n",
    "        # First  calculate the strength of the input signal.\n",
    "        strength = np.dot(values, self.weights)\n",
    "        self.last_input = strength\n",
    "        \n",
    "        # Modify strength using the sigmoid activation function and return as output signal.\n",
    "        # Hint: You may want to create a helper function to compute the logistic function since\n",
    "        # you will need it for the update function.\n",
    "        result = 1.0 / ( 1 + np.exp(-strength))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def logistic(self, strength):\n",
    "        return 1 / (1 + np.exp(-strength))  \n",
    "\n",
    "    def update(self, values, train, eta=.1):\n",
    "        '''\n",
    "        Take in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected outputs.\n",
    "        Updates internal weights according to gradient descent using these values and \n",
    "        an optional learning rate, @param eta.\n",
    "        '''\n",
    "        \n",
    "        # for each data point\n",
    "        for X, y_true in zip(values, train):  # (values, train)\n",
    "            y_pred = self.activate(X)\n",
    "            \n",
    "            # compute derivative of logistic function at input strength \n",
    "            # Recall: d/dx logistic(x) = logistic(x) * (1- logistic(x))\n",
    "            dx = self.logistic(self.last_input) * (1 - self.logistic(self.last_input))\n",
    "            \n",
    "            # update self.weights based on learning rate, signal accuracy,\n",
    "            # function slope (derivative) and input value\n",
    "            self.weights += eta * (y_true - y_pred) * dx*X\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-5):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    u1 = Sigmoid(weights=[3,-2,1])\n",
    "    assert abs(u1.activate(np.array([1,2,3])) - 0.880797) < 1e-5\n",
    "    \n",
    "    u1.update(np.array([[1,2,3]]),np.array([0]))\n",
    "    assert sum_almost_equal(u1.weights, np.array([2.990752, -2.018496, 0.972257]))\n",
    "\n",
    "    u2 = Sigmoid(weights=[0,3,-1])\n",
    "    u2.update(np.array([[-3,-1,2],[2,1,2]]),np.array([1,0]))\n",
    "    assert sum_almost_equal(u2.weights, np.array([-0.030739, 2.984961, -1.027437]))\n",
    "\n",
    "    print('Test End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test End\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
