{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络：感知机Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机Perceptron是一种**人工神经元**。   \n",
    "在现代的神经网络工作中，更常用的是另一种人工神经元--**sigmoid神经元**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udacity：\n",
    "![title](../images/perceptron_u.png)\n",
    "\n",
    "图解：   \n",
    "激活函数activation function： $${\\sum_{i=1}^k}x_i \\cdot w_i$$\n",
    "阈值firing threshold：$\\theta$\n",
    "符号函数sign function：$$sign(x) = \\begin{cases} +1, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases}$$\n",
    "\n",
    "在图中的例子$x_1 = 1, x_2 = 0, x_3 = -1.5$，$w_1 = 1/2, w_2 = 3/5, w_3 = 1$，\n",
    "求得的结果为-1，-1小于0，所以$y = 0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 感知器得出的都是线性关系，所有的感知器都将计算线性问题，始终都将计算半平面halfplanes。\n",
    "感知器就是一个线性函数，它计算的是超平面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAND门（与非门）可以用来做基本逻辑运算。   \n",
    "事实上，我们可以使用感知机网络做任何逻辑运算。   \n",
    "因为**与非门**是通用的计算，我们可以用与非门实现任何运算。\n",
    "\n",
    "<a href = 'https://www.jianshu.com/p/921e9c6be305'>与非门示例</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Trainging:\n",
    "Given examples, find weights that map inputs to outputs.\n",
    "- perceptron rule   (threshold)\n",
    "- gradient descent / delta rule ( unthreshold)\n",
    "\n",
    "感知机训练法则在**样本线性可分**时，能够找到最佳的权重向量，使得所有样本均被正确分类。    \n",
    "如果**样本线性不可分**，则不能收敛。    \n",
    "\n",
    "不能收敛，也无法找到一个权向量使得分类错误率最小吗？   \n",
    "\n",
    "而采用梯度下降法，能够在训练样本线性不可分的情况下，找到目标的最佳近似，即分类错误率最小。   \n",
    "\n",
    "#### 感知机训练法则使得线性可分的样本集得到完全正确的分类（完美拟合）。\n",
    "#### 梯度下降法，在线性可分时能够达到一定的效果（全部正确分类），而且在线性不可分的时候，能够找到使分类错误率最小的最佳拟合。\n",
    "\n",
    "<a href='https://blog.csdn.net/bingduanlbd/article/details/24468885'>参考博文</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perceptron rule (threshold)\n",
    "![title](../images/Perceptron_rule.png)\n",
    "\n",
    "\n",
    "\n",
    "## gradient descent / delta rule ( unthreshold)\n",
    "![title](../images/Gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 上图理解：\n",
    "- 第一张图，Perceptron Rule，我们仅针对$y-\\hat y$去的到错误的结果数，只考虑了错误数对参数的影响。\n",
    "- 第二张图通过求导解释了$L2$范数$\\frac{1}{2}$对结果的影响。\n",
    "\n",
    "两个例子的区别：\n",
    "感知机规则的例子中进行了阈值化，而在另一个例子梯度下降中未进行阈值化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01 -0.01]\n",
      "[-0.02 -0.02]\n",
      "[0. 0.]\n",
      "[0. 0.]\n",
      "[0.01 0.01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "class Perceptron1(object):\n",
    "    '''\n",
    "    根据Perceptron Rule训练感知机模型\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    \n",
    "    def _recursion(x, diffs):\n",
    "        for idx, d in enumerate(list(diffs)):\n",
    "                delta = lr * d * x[idx]\n",
    "                self.w[idx] += delta\n",
    "        \n",
    "    def fit(self, x, y, lr=0.01, epoch=1):\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "       \n",
    "        self.w = np.zeros(x.shape[1])\n",
    "        \n",
    "        self.b = 0\n",
    "        \n",
    "        for _ in range(epoch):\n",
    "            \n",
    "            y_pred = np.dot(x, self.w) + self.b\n",
    "            for idx, e in enumerate(y_pred):\n",
    "                if e >=0:\n",
    "                    y_pred[idx] = 1\n",
    "                else:\n",
    "                    y_pred[idx] = -1\n",
    "               \n",
    "            diffs = y - y_pred\n",
    "            diffs = diffs[:, 0]\n",
    "        \n",
    "            # print(diffs.sum())\n",
    "           \n",
    "            if diffs.sum() == 0:\n",
    "                print('END：', self.w, self.b)\n",
    "            else:\n",
    "                \n",
    "                for idx, d in enumerate(list(diffs)):\n",
    "                    # 单个的更新有问题\n",
    "                    delta = lr * d * x[idx]\n",
    "                    print(delta)\n",
    "                    self.w += delta\n",
    "                    self.b += lr*d\n",
    "\n",
    "            \n",
    "perceptron_obj = Perceptron1()\n",
    "X_train = [[1,1], [2,2], [3,3], [4,4], [-1,-1]]\n",
    "y_train = [[0], [0], [1], [1], [0]]\n",
    "perceptron_obj.fit(X_train, y_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.00000000e-02 -1.56125113e-17 -1.56125113e-17]\n"
     ]
    }
   ],
   "source": [
    "class Perceptron(object):\n",
    "    \"\"\"Perceptron classifier.\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta:float,Learning rate (between 0.0 and 1.0)\n",
    "    n_iter:int,Passes over the training dataset.\n",
    "     \n",
    "    Attributes\n",
    "    -------------\n",
    "    w_: 1d-array,Weights after fitting.\n",
    "    errors_: list,Numebr of misclassifications in every epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self,eta=0.01,n_iter=10):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"Fit training data.先对权重参数初始化，然后对训练集中每一个样本循环，根据感知机算法学习规则对权重进行更新\n",
    "        Parameters\n",
    "        ------------\n",
    "        X: {array-like}, shape=[n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and n_featuers is the number of features.\n",
    "        y: array-like, shape=[n_smaples]\n",
    "            Target values.\n",
    "        Returns\n",
    "        ----------\n",
    "        self: object\n",
    "        \"\"\"\n",
    "        self.w_ = np.zeros(1 + X.shape[1]) # add w_0　　　　　#初始化权重。数据集特征维数+1。\n",
    "        self.errors_ = []#用于记录每一轮中误分类的样本数\n",
    "         \n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X,y):\n",
    "                update = self.eta * (target - self.predict(xi))#调用了predict()函数\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "     \n",
    "    def net_input(self,X):\n",
    "        \"\"\"calculate net input\"\"\"\n",
    "        return np.dot(X,self.w_[1:]) + self.w_[0]#计算向量点乘\n",
    "    \n",
    "    def predict(self,X):#预测类别标记\n",
    "        \"\"\"return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0,1,-1)\n",
    "\n",
    "perceptron_obj = Perceptron()\n",
    "X_train = np.array([[1,1], [2,2], [3,3], [4,4], [-1,-1]])\n",
    "y_train = np.array([[0], [0], [1], [1], [0]])\n",
    "model = perceptron_obj.fit(X_train, y_train) \n",
    "print(model.w_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**梯度下降法**：\n",
    "- 第一步：求损失函数的梯度（求导）\n",
    "- 第二步：梯度是函数值增长最快的方向，我们想要最小化损失函数，想要让函数值减小得最快，就是将参数沿着梯度的反方向走一步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "print('+'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 李航：\n",
    "感知机（perceptron）是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取 +1 和 -1 二值。   \n",
    "感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。   \n",
    "感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。   \n",
    "感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。    \n",
    "感知机预测是用学习得到的感知机模型对新的输入实例进行分类。  \n",
    "感知机1957年由Rosenblatt提出，是神经网络与支持向量机的基础。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机模型\n",
    "感知机定义：   \n",
    "假设输入空间（特征空间）$X \\subseteq R^{n}$，输出空间是$y =\\{+1, -1\\}$。输入$x\\in{X}$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y\\in Y$表示实例的类别。   \n",
    "由输入空间到输出空间的函数：\n",
    "$$f(x) = sign(w\\cdot x +b)$$ 称为感知机。   \n",
    "\n",
    "其中，$w$和$b$为感知机模型参数，$w\\in R^{n}$叫做权值（weight）或权值向量(weight vector)，$b\\in R$叫做偏置(bias)，     \n",
    "$w \\cdot x$表示$w$和$x$的内积。    \n",
    "sign是符号函数，即$$sign(x) = \\begin{cases} +1, & x \\ge 0 \\\\ -1, & x < 0 \\end{cases}$$\n",
    "\n",
    "\n",
    "感知机是一种线性分类模型，属于判别模型。   \n",
    "感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier），即函数集合$\\{ f | f(x) = w \\cdot x + b\\}$。\n",
    "\n",
    "\n",
    "感知机的几何解释：  \n",
    "线性方程：$$w \\cdot x + b = 0$$\n",
    "对应于特征空间$R^{n}$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距。 这个超平面将特征空间划分为两个部分。   \n",
    "位于两部分的点（特征向量）分别被分为正、负两类。   \n",
    "因此，超平面$S$称为分离超平面(separating hyperplane)。\n",
    "\n",
    "![title](../images/perceptron_model1.png)\n",
    "\n",
    "\n",
    "感知机学习，由训练数据集（实例的特征向量及类别）  \n",
    "$$ T = \\{(x_1，y_1), (x_2, y_2), ... , (x_N, y_N)\\}$$   \n",
    "其中，$x_i \\in X = R^n$，$y_i \\in Y = \\{+1, -1\\}$，$i = 1,2,...,N$，求得感知机模型，即求得模型参数$w$，$b$。   \n",
    "感知机预测，通过学习得到的感知机模型，对于新年的输入实例给出其对应的输出类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机的学习策略（损失函数）\n",
    "#### 1.数据集的线性可分性：\n",
    "给定一个数据集$$T=\\{(x_1, y_1), (x_2, y_2), ... , (x_N, y_N)\\}$$\n",
    "其中，$x_i \\in X = R^n$, $y_i in Y = \\{+1, -1\\}$, $i = 1,2,...,N$,  如果存在某个超平面$S$\n",
    "$$w \\cdot x + b = 0$$\n",
    "能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i = +1 $的实例$i$，有$w \\cdot x_i + b > 0$，对所有$y_i = -1$的实例$i$，有$w \\cdot x_i + b < 0$，则称数据集$T$为线性可分数据集（linear separable data set）；否则，称数据集$T$线性不可分。\n",
    "\n",
    "\n",
    "#### 2.感知机学习策略\n",
    "假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。    \n",
    "为了找出这样的超平面，即确定感知机模型参数$w$，$b$，需要确定一个学习策略，即**定义（经验）损失函数并将损失函数最小化。**      \n",
    "\n",
    "**损失函数：**\n",
    "- 损失函数的一个自然选择是误分类点的总数。   \n",
    "    - 但是，这样的损失函数不是参数$w$，$b$的连续可导函数，不易优化。    \n",
    "- 损失函数的另一个选择是误分类点到超平面$S$的总距离，这是感知机所采用的。\n",
    "    - 为此，首先写出输入空间$R^n$中任一点$x_0$到超平面$S$的距离：\n",
    "        $$\\frac{1}{||w||}|w \\cdot x_0 + b |$$\n",
    "        这里，$||w||$是$w$的$L_2$范数。\n",
    "    - 其次，对于误分类的数据$(x_i, y_i)$来说：\n",
    "        $$-y_i(w \\cdot x_i + b ) > 0$$\n",
    "    - 这样，假设超平面$S$的误分类点的集合为$M$，那么所有误分类点到超平面$S$的总距离为\n",
    "        $$- \\frac{1}{||w||} \\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$\n",
    "    - 不考虑$- \\frac{1}{||w||}$，就得到感知机学习的损失函数。\n",
    "    \n",
    "由上可得，给定训练数据集\n",
    "$$ T = \\{(x_1, y_1), (x_2, y_2), ... , (x_N, y_N)\\}$$\n",
    "其中，$x_i \\in X = R^n$，$y_i \\in Y = \\{ +1, -1\\}$，$i = 1,2,...,N$。感知机$sign(w \\cdot x + b)$学习的损失函数定义为\n",
    "$$L(w, b) = -\\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$\n",
    "其中$M$为误分类点的集合。   \n",
    "这个损失函数就是感知机学习的经验风险函数。    \n",
    "\n",
    "显然，损失函数$L(w,b)$是非负的。    \n",
    "如果没有误分类点，损失函数值是0。   \n",
    "而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。   \n",
    "\n",
    "一个特定的样本点的损失函数：   \n",
    "在误分类时是参数$w$，$b$的线性函数，在正确分类时是0。   \n",
    "因此，给定训练数据集$T$，损失函数$L(w,b)$是$w$，$b$的连续可导函数。   \n",
    "\n",
    "感知机学习的策略是在假设空间中选取使损失函数$$L(w, b) = -\\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$最小的模型参数$w$，$b$，即感知机模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机学习算法：原始形式、对偶形式\n",
    "感知机学习问题转化为求解损失函数$$L(w, b) = -\\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$的最优化问题，最优化的方法是随着梯度下降法。        \n",
    "\n",
    "关于感知机学习的具体算法：\n",
    "- 原始形式\n",
    "- 对偶形式\n",
    "\n",
    "\n",
    "### 1.感知机学习算法的原始形式\n",
    "感知机学习算法是对以下最优化问题的算法。   \n",
    "给定一个训练数据集\n",
    "$$ T = \\{(x_1, y_1), (x_2, y_2), ... , (x_N, y_N)\\}$$\n",
    "其中，$x_i \\in X = R^n$，$y_i \\in Y = \\{ +1, -1\\}$，$i = 1,2,...,N$。\n",
    "求参数$w$，$b$，使其为以下损失函数极小化问题的解\n",
    "$$\\min_{w,b} L(w, b) = -\\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$\n",
    "其中，$M$为误分类点的集合。  \n",
    "\n",
    "感知机学习算法时误分类驱动的，具体采用随机梯度下降法（stochastic gradient descent）。    \n",
    "首先，任意选取一个超平面$w_0$，$b_0$，然后用梯度下降法不断地极小化目标函数$$\\min_{w,b} L(w, b) = -\\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$\n",
    "极小化过程中不是一次使$M$中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。   \n",
    "\n",
    "假设误分类点集合$M$是固定的，那么损失函数$L(w,b)$的梯度由\n",
    "$$\\nabla_w L(w,b) = - \\sum_{x_i \\in M} y_i x_i$$\n",
    "$$\\nabla_b L(w,b) = - \\sum_{x_i \\in M} y_i$$\n",
    "给出。\n",
    "\n",
    "\n",
    "随机选取一个误分类点$(x_i, y_i)$，对$w$，$b$进行更新：\n",
    "$$w \\leftarrow w + \\eta y_i x_i$$\n",
    "$$b \\leftarrow b + \\eta y_i$$\n",
    "式中$\\eta( 0 < \\eta \\leq 1）$是步长，在统计学习中又称学习率（learning rate）。   \n",
    "这样，通过迭代可以期待损失函数$L(w,b)$不断减小，直到为0。     \n",
    "\n",
    "\n",
    "<p style='color:blue;font-size:22px;'>算法：感知机学习算法的原始形式</p>\n",
    "\n",
    "**输入：**训练数据集$T = \\{(x_1, y_1), (x_2, y_2), ... , (x_N, y_N)\\}$，其中，$x_i \\in X = R^n$，$y_i \\in Y = \\{ +1, -1\\}$，$i = 1,2,...,N$，学习率$\\eta( 0 < \\eta \\leq 1$；   \n",
    "**输出：**$w$，$b$；感知机模型$f(x) = sign(w \\cdot x +b)$     \n",
    "- 选取初值$w_0$，$b_0$；\n",
    "- 在训练集中选取数据$(x_i, y_i)$\n",
    "- 如果$y_i (w \\cdot x_i + b) \\leq 0 $\n",
    "$$w \\leftarrow w + \\eta y_i x_i$$\n",
    "$$b \\leftarrow b + \\eta y_i$$\n",
    "- 回到第二步，直至训练集中没有误分类点。  \n",
    "\n",
    "这种学习算法直观上有如下解释：   \n",
    "当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整$w$，$b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'>假设误分类点集合$M$是固定的，那么损失函数$L(w,b)$的梯度由\n",
    "$$\\nabla_w L(w,b) = - \\sum_{x_i \\in M} y_i x_i$$\n",
    "$$\\nabla_b L(w,b) = - \\sum_{x_i \\in M} y_i$$\n",
    "给出\n",
    "++++++++ 此处不理解，为啥梯度是这么算出来的！！！微积分快点补！！！！大笨蛋！！！！</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 算法的收敛性\n",
    "证明见书籍，目前微积分未补的情况下，难理解。\n",
    "\n",
    "对偶形式的算法在SVM期间对应了一起看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
