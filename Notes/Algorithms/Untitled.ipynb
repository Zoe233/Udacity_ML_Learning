{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络：感知机Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机Perceptron是一种**人工神经元**。   \n",
    "在现代的神经网络工作中，更常用的是另一种人工神经元--**sigmoid神经元**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udacity：\n",
    "![title](../images/perceptron_u.png)\n",
    "\n",
    "图解：   \n",
    "激活函数activation function： $${\\sum_{i=1}^k}x_i \\cdot w_i$$\n",
    "阈值firing threshold：$\\theta$\n",
    "符号函数sign function：$$sign(x) = \\begin{cases} +1, & x \\ge 0 \\\\ 0, & x < 0 \\end{cases}$$\n",
    "\n",
    "在图中的例子$x_1 = 1, x_2 = 0, x_3 = -1.5$，$w_1 = 1/2, w_2 = 3/5, w_3 = 1$，\n",
    "求得的结果为-1，-1小于0，所以$y = 0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 感知器得出的都是线性关系，所有的感知器都将计算线性问题，始终都将计算半平面halfplanes。\n",
    "感知器就是一个线性函数，它计算的是超平面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAND门（与非门）可以用来做基本逻辑运算。   \n",
    "事实上，我们可以使用感知机网络做任何逻辑运算。   \n",
    "因为**与非门**是通用的计算，我们可以用与非门实现任何运算。\n",
    "\n",
    "<a href = 'https://www.jianshu.com/p/921e9c6be305'>与非门示例</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Trainging:\n",
    "Given examples, find weights that map inputs to outputs.\n",
    "- perceptron rule   (threshold)\n",
    "- gradient descent / delta rule ( unthreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perceptron rule (threshold)\n",
    "![title](../images/Perceptron_rule.png)\n",
    "\n",
    "\n",
    "\n",
    "## gradient descent / delta rule ( unthreshold)\n",
    "![title](../images/Gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 上图理解：\n",
    "- 第一张图，Perceptron Rule，我们仅针对$y-\\hat y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义感知机训练模型示例\n",
    "\n",
    "感知机模型只有$w$和$b$两个参数，其中$w$是一个n维向量（$w \\in R^n$），$b$是一个标量（$b \\in R$）。   \n",
    "为了保证收敛性，我们需要将$w$初始化为零向量，将$b$初始化为0。\n",
    "\n",
    "<a href = 'https://www.leiphone.com/news/201706/QFydbeV7FXQtRIOl.html'>来源</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class Perceptron:\n",
    "    '''\n",
    "    自定义感知机训练模型\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self._w = None\n",
    "        self._b = None\n",
    "    \n",
    "    def fit(self, x, y, lr=0.01, epoch=1000):\n",
    "        x, y = np.asarray(x, np.float32), np.asarray(y, np.float32)\n",
    "        self._w = np.zeros(x.shape[1])\n",
    "        self._b = 0 \n",
    "        \n",
    "        for _ in range(epoch):\n",
    "            y_pred = x.dot(self._w) + self._b  # 计算 w·x+b\n",
    "            idx = np.argmax(np.maximum(0, -y_pred * y))  # 求所有y_true和y_pred的结果不一致的y的索引列表\n",
    "           \n",
    "            if y[idx] * y_pred[idx] > 0:\n",
    "                break\n",
    "            delta = lr * y[idx]\n",
    "            self._w += delta * w[idx]\n",
    "            self._b += delta\n",
    "        print(self._w)\n",
    "        print(self._b)\n",
    "\n",
    "perceptron_obj = Perceptron()\n",
    "X_train = [[1,1], [2,2], [3,3], [4,4]]\n",
    "y_train = [[0], [0], [1], [1]]\n",
    "perceptron_obj.fit(X_train, y_train)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**梯度下降法**：\n",
    "- 第一步：求损失函数的梯度（求导）\n",
    "- 第二步：梯度是函数值增长最快的方向，我们想要最小化损失函数，想要让函数值减小得最快，就是将参数沿着梯度的反方向走一步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 李航：\n",
    "感知机（perceptron）是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取 +1 和 -1 二值。   \n",
    "感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。   \n",
    "感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。   \n",
    "感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。    \n",
    "感知机预测是用学习得到的感知机模型对新的输入实例进行分类。  \n",
    "感知机1957年由Rosenblatt提出，是神经网络与支持向量机的基础。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机模型\n",
    "感知机定义：   \n",
    "假设输入空间（特征空间）$X \\subseteq R^{n}$，输出空间是$y =\\{+1, -1\\}$。输入$x\\in{X}$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y\\in Y$表示实例的类别。   \n",
    "由输入空间到输出空间的函数：\n",
    "$$f(x) = sign(w\\cdot x +b)$$ 称为感知机。   \n",
    "\n",
    "其中，$w$和$b$为感知机模型参数，$w\\in R^{n}$叫做权值（weight）或权值向量(weight vector)，$b\\in R$叫做偏置(bias)，     \n",
    "$w \\cdot x$表示$w$和$x$的内积。    \n",
    "sign是符号函数，即$$sign(x) = \\begin{cases} +1, & x \\ge 0 \\\\ -1, & x < 0 \\end{cases}$$\n",
    "\n",
    "\n",
    "感知机是一种线性分类模型，属于判别模型。   \n",
    "感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier），即函数集合$\\{ f | f(x) = w \\cdot x + b\\}$。\n",
    "\n",
    "\n",
    "感知机的几何解释：  \n",
    "线性方程：$$w \\cdot x + b = 0$$\n",
    "对应于特征空间$R^{n}$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距。 这个超平面将特征空间划分为两个部分。   \n",
    "位于两部分的点（特征向量）分别被分为正、负两类。   \n",
    "因此，超平面$S$称为分离超平面(separating hyperplane)。\n",
    "\n",
    "![title](../images/perceptron_model1.png)\n",
    "\n",
    "\n",
    "感知机学习，由训练数据集（实例的特征向量及类别）  \n",
    "$$ T = \\{(x_1，y_1), (x_2, y_2), ... , (x_N, y_N)\\}$$   \n",
    "其中，$x_i \\in X = R^n$，$y_i \\in Y = \\{+1, -1\\}$，$i = 1,2,...,N$，求得感知机模型，即求得模型参数$w$，$b$。   \n",
    "感知机预测，通过学习得到的感知机模型，对于新年的输入实例给出其对应的输出类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机的学习策略（损失函数）\n",
    "#### 1.数据集的线性可分性：\n",
    "给定一个数据集$$T=\\{(x_1, y_1), (x_2, y_2), ... , (x_N, y_N)\\}$$\n",
    "其中，$x_i \\in X = R^n$, $y_i in Y = \\{+1, -1\\}$, $i = 1,2,...,N$,  如果存在某个超平面$S$\n",
    "$$w \\cdot x + b = 0$$\n",
    "能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i = +1 $的实例$i$，有$w \\cdot x_i + b > 0$，对所有$y_i = -1$的实例$i$，有$w \\cdot x_i + b < 0$，则称数据集$T$为线性可分数据集（linear separable data set）；否则，称数据集$T$线性不可分。\n",
    "\n",
    "\n",
    "#### 2.感知机学习策略\n",
    "假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。    \n",
    "为了找出这样的超平面，即确定感知机模型参数$w$，$b$，需要确定一个学习策略，即**定义（经验）损失函数并将损失函数最小化。**      \n",
    "\n",
    "**损失函数：**\n",
    "- 损失函数的一个自然选择是误分类点的总数。   \n",
    "    - 但是，这样的损失函数不是参数$w$，$b$的连续可导函数，不易优化。    \n",
    "- 损失函数的另一个选择是误分类点到超平面$S$的总距离，这是感知机所采用的。\n",
    "    - 为此，首先写出输入空间$R^n$中任一点$x_0$到超平面$S$的距离：\n",
    "        $$\\frac{1}{||w||}|w \\cdot x_0 + b |$$\n",
    "        这里，$||w||$是$w$的$L_2$范数。\n",
    "    - 其次，对于误分类的数据$(x_i, y_i)$来说：\n",
    "        $$-y_i(w \\cdot x_i + b ) > 0$$\n",
    "    - 这样，假设超平面$S$的误分类点的集合为$M$，那么所有误分类点到超平面$S$的总距离为\n",
    "        $$- \\frac{1}{||w||} \\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$\n",
    "    - 不考虑$- \\frac{1}{||w||}$，就得到感知机学习的损失函数。\n",
    "    \n",
    "由上可得，给定训练数据集\n",
    "$$ T = \\{(x_1, y_1), (x_2, y_2), ... , (x_N, y_N)\\}$$\n",
    "其中，$x_i \\in X = R^n$，$y_i \\in Y = \\{ +1, -1\\}$，$i = 1,2,...,N$。感知机$sign(w \\cdot x + b)$学习的损失函数定义为\n",
    "$$L(w, b) = -\\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机学习算法：原始形式、对偶形式\n",
    "### 算法的收敛性"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
