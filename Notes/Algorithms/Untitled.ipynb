{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习中出现的非常频繁的问题：**过拟合**和**规则化（又名：正则化）**。\n",
    "\n",
    "此篇整理文档主要用于理解常用的L0、L1、L2和核范数规则化，还有规则化项参数的选择问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习的问题无非就是“minimize error while regularing you parameters”，也就是在正则化参数的同时最小化误差。   \n",
    "\n",
    "**最小化误差**是为了让我们的模型拟合我们的训练数据，而**正则化参数**是防止我们的模型过分拟合我们的训练数据。   \n",
    "\n",
    "因为参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。   \n",
    "但训练误差小并不是我们的最终目标，我们的目标是希望模型的测试误差小，也就是你能够准确的预测新的样本。  \n",
    "所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能，而模型“简单”就是通过规则函数实现的。   \n",
    "\n",
    "<p style='color:blue;'>注意，在之前整理的“5偏差和方差”中，简单的定义是需要结合现实来看的。<a href='./5偏差和方差.ipynb'>5偏差和方差</a></p>\n",
    "\n",
    "\n",
    "另外，规则项的使用还可以约束我们的模型的特性。   \n",
    "这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低轶、平滑等等。   \n",
    "要知道，有时候人的先验是非常重要的。  \n",
    "由于人和机器的交流目前没有和人和人交流的那么直接的方法，目前这个媒介只能由规则项来担当了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有几种角度来看规则化的。  \n",
    "规划则符合**奥卡姆剃刀(Occam's razor)原理**。   \n",
    "思想：   \n",
    "在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型。   \n",
    "\n",
    "从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。   \n",
    "\n",
    "还有个说法就是，规则化是**结构风险最小化策略**的实现，是在经验风险上加一个正则化项（regularizer）或惩罚项（penalty term）。\n",
    "\n",
    "一般来说，监督学习可以看做最小化下面的目标函数：\n",
    "$$w^* = arg min_{w} \\sum_{i} L(y_i, f(x_i; w)) + \\lambda \\omega (w)$$\n",
    "\n",
    "其中，第一项$L(y_i, f(x_i; w))$衡量我们的模型（分类或者回归）对第$i$个样本的预测值$f(x_i; w)$和真实的标签$y_i$之前的误差。  \n",
    "因为我们的模型是要拟合我们的训练样本，所以我们要求这一项最小，也就是要求我们的模型尽量的拟合我们的训练数据。   \n",
    "\n",
    "但正如上面所言，我们不仅要保证训练误差最小，我们更希望我们的模型测试误差小，所以我们需要加上第二项，也就是对参数$w$的规则化函数$\\omega(w)$去约束我们的模型尽量的简单。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的大部分带参模型都和这个不但形似，而且神似。   \n",
    "其实大部分无非就是变换这两项而已。  \n",
    "\n",
    "对于第一项Loss Function，如果是Square Loss，那就是最小二乘；  \n",
    "如果是Hinge Loss，那就是著名的SVM了；   \n",
    "如果是exp-Loss，那就是牛逼的Bossting了；   \n",
    "如果是log-Loss，那就是Logistic Regression了；  \n",
    "......    \n",
    "\n",
    "不同的loss函数，具有不同的拟合特性，这个也得具体问题具体分析。   \n",
    "但是，我们先不研究loss function的问题，我们把目光转向“规则项$\\omega(w)$”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "规则化函数$\\omega(w)$也有很多选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化项就越大。   \n",
    "比如，规则化项可是模型参数向量的范数。   \n",
    "然而，不同的选择参数$w$的约束不同，取得的效果也不同。   \n",
    "\n",
    "我们常见的都聚集在： \n",
    "- 零范数 L0\n",
    "- 一范数 L1\n",
    "- 二范数 L2\n",
    "- 迹范数\n",
    "- Frobenius范数\n",
    "- 核范数\n",
    "\n",
    "那么多范数，到底它们表达了什么意思？  具有什么能力？  什么时候才能用？ 什么时候需要用呢？  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L0范数和L1范数\n",
    "L0范数是指向量中非0的元素的个数。   \n",
    "如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。   \n",
    "换句话说，让参数W是稀疏的。  \n",
    "\n",
    "我们知道非零元素的零次方为1，但零的零次方，非零数开零次方都是什么鬼，很不好说明L0的意义，所以在通常情况下，大家都用的是：\n",
    "$$||x_0||_0 = \\# (i| x_i \\ne {0})$$ \n",
    "表示向量$x$中的非零元素的个数。  \n",
    "\n",
    "对于L0范数，其优化问题为：\n",
    "$$min ||x_0||_0$$\n",
    "$$s.t. Ax = b $$\n",
    "\n",
    "在实际应用中，由于L0范数本身不容易有一个好的数学表示形式，给出上面问题的形式化表示是一个很难的问题，故被人认为是一个NP难问题。  \n",
    "所以，在实际情况中，L0的最优问题会被放宽到L1或L2的最优化。\n",
    "\n",
    "\n",
    "针对上面这句“L0范数让参数W是稀疏的”，看到“稀疏”二字，大家都应该从当下风风火火的“压缩感知”和“稀疏编码”中醒悟过来，原来“稀疏”就是通过L0范数实现的。   \n",
    "但是，在大部分papers中，稀疏都是通过L1范数来实现的。   \n",
    "-- L0和L1有着某种不寻常的关系，所以这里将L0范数和L1范数一起来讲了。  \n",
    "\n",
    "- L1范数是什么？\n",
    "- 它为什么可以实现稀疏？\n",
    "- 为什么大家都用L1范数去实现稀疏，而不是L0范数呢？\n",
    "\n",
    "### L1范数是什么？\n",
    "L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso Regularization）。\n",
    "\n",
    "### 为什么L1范数会使权值稀疏？\n",
    "有人可能会这样给你回答“它是L0范数的最优凸近似”。   \n",
    "实际上，还存在一个更美的回答：  \n",
    "任何的规则化算子，如果他在$W_i = 0 $的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。  \n",
    "\n",
    "W的L1范数是绝对值，$|w|$在w=0处不可微，但这还是不够直观。  \n",
    "这里因为我们需要和L2范数进行对比分析。\n",
    "\n",
    "### 既然L0可以实现稀疏，为什么不用L0，而要用L1呢？  \n",
    "- 因为L0范数很难优化求解（NP难问题）\n",
    "- L1范数是L0范数的最优凸近似，而且它比L0范数要更容易优化求解。\n",
    "![title](../images/l0_l1.png)\n",
    "\n",
    "总结：  \n",
    "L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性被而广泛地应用。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
