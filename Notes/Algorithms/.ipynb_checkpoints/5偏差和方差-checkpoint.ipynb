{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inductive Bias 归纳偏置\n",
    "以**决策树**为例：    \n",
    "较短的树比较长的树优先，那些信息增益高的属性更靠近节点的树优先。   \n",
    "决策树的搜索范围是一个完整的假设空间，但它不彻底搜索这个空间。   \n",
    "从简单的假设到复杂的假设，直到遇到终止条件。   \n",
    "\n",
    "**变性空间候选消除算法**正和决策树相反，假设空间不完整，但搜索整个假设空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Bias 优先偏置\n",
    "定义某种假设（最短树）胜过其他假设的一种优选，对最终假设没有硬性假设。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restriction Bias 限定偏置\n",
    "对待考虑假设的一种限定。   \n",
    "通常优选偏置比限定偏置更符合一般假设，因为允许在学习器工作在完整的假设空间保证了未知目标函数被包含在内。   \n",
    "相反限定偏置严格限定假设集合的潜在空间。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 奥坎姆剃刀\n",
    "优先选择拟合数据的最简单假设。  \n",
    "是一个应用很广的**归纳偏置**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过度拟合\n",
    "对于一个假设，当存在其他假设对训练样例的拟合比它差，但是事实上在实例的整个分布上表现更好，我们就说这个假设为过度拟合。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 周志华-西瓜书《机器学习》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归纳偏好 Inductive Bias\n",
    "通过学习得到的模型对应了假设空间中的一个假设。    \n",
    "\n",
    "![title](../images/watermelon.png)\n",
    "\n",
    "上图1.2的西瓜版本空间给我们带来了一个麻烦：    \n",
    "> 现在有三个训练集一致的假设，但与它们对应的模型在面临新样本的时候，却会产生不同的输出。   \n",
    "    例如：    \n",
    "    对（色泽=青绿；根蒂=蜷缩；敲声=沉闷）这个新收来的挂，如果我们采用的是\"好瓜$\\leftrightarrow$(色泽= *）$\\bigwedge$(根蒂=蜷缩)$\\bigwedge$(敲声=*)\"，那么将会把新瓜判断为好瓜，而如果采用另外两个假设，则判断的结果将不会是好瓜。\n",
    "    \n",
    "    那么，应该采用哪一个模型（或假设）呢？\n",
    "    \n",
    "    \n",
    "    \n",
    "若仅有表1.1中的训练样本，\n",
    "![title](../images/watermelon_table.png)\n",
    "则无断定上述三个假设中哪一个“更好”，然而，对于一个具体的学习算法而言，它必须要产生一个模型。   \n",
    "\n",
    "这时，学习算法本身的“偏好”就会起到关键的作用。   \n",
    "\n",
    "例如，若我们的算法喜欢“尽可能特殊”的模型，则它会选择\"好瓜$\\leftrightarrow$(色泽= *）$\\bigwedge$(根蒂=蜷缩)$\\bigwedge$(敲声=*)\"。   \n",
    "**机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”（inductive bias）,或简称为“偏好”。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上“等效”的假设所迷惑，而无法产生确定的学习结果。    \n",
    "\n",
    "可以想象，如果没有偏好，我们的西瓜学习算法产生的模型每次在进行预测时随机抽取训练集上的等效假设，那么对这个新瓜“（色泽=青绿；根蒂=蜷缩；敲声=沉闷）”，学来的模型时而告诉我们它是好的、时而告诉我们它是不好的，这样的学习结果显然没有意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归纳偏好的作用在图1.3这个回归学习图示中可能更加直观。  \n",
    "这里的训练样本时图中的一个点$(x, y)$，要学得一个与训练集一致的模型，相当于找到一条穿过所有训练样本点的曲线。  \n",
    "![title](../images/regression_01.png)\n",
    "\n",
    "显然，对有限个样本点组成的训练集，存在着很多条曲线与其一致。   \n",
    "我们的学习算法必须要某种偏好，才能产生它认为“正确”的模型。   \n",
    "> 例如，若认为相似的样本应有相似的输出（例如，在各种属性上都很相像的西瓜，成熟程度应该比较接近），则对应的学习算法可能偏好图1.3中比较“平滑”的曲线A而不是比较“崎岖”的曲线B。\n",
    "\n",
    "归纳偏好可看作学习算法自身在一个很庞大的假设空间中对假设进行选择的启发式或“价值观”。  \n",
    "\n",
    "那么，有没有一般性的原则来引导算法确立“正确的”偏好呢？   \n",
    "### “奥卡姆剃刀”（Occam's razor）\n",
    "> 是一种常用的、自然科学研究中最基本的原则。\n",
    "> 即“若有多个假设与观察一致，则选最简单的那个”。\n",
    "\n",
    "如果采用这个原则，并且假设我们认为“更平滑”意味着“更简单”。   \n",
    "例如曲线A更易于描述，其方程式$y=-x^2 + 6x +1 $，而曲线B则要复杂得多。则在图1.3中我们会自然地偏好“平滑”的曲线A。   \n",
    "\n",
    "然而，奥卡姆剃刀并非唯一可行的原则。   \n",
    "退一步说，即便假定我们是奥卡姆剃刀的铁杆拥趸，也许注意到，奥卡姆剃刀本身存在不同的诠释，使用奥卡姆剃刀原则并不平凡。   \n",
    "例如，对我们已经很熟悉的西瓜问题来说：\n",
    "- 假设1：\"好瓜$\\leftrightarrow$（色泽=*）$\\bigwedge$（根蒂=蜷缩）$\\bigwedge$(敲声=浊响)\"\n",
    "- 假设2：\"好瓜$\\leftrightarrow$（色泽=\\*）$\\bigwedge$（根蒂=蜷缩）$\\bigwedge$(敲声= \\*)\"\n",
    "\n",
    "这两个假设，哪一个更“简单”呢？   \n",
    "这个问题并不简单，需借助其他机制才能解决。     \n",
    "\n",
    "事实上，归纳偏好对应了学习算法本身所做出的关于“什么样的模型更好”的假设。   \n",
    "在具体的现实问题中，这个假设是否成立，即算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好的性能。   \n",
    "\n",
    "\n",
    "让我们再回头看看图1.3。   \n",
    "假设学习算法$\\Im_a$基于某种归纳偏好产生了对应于曲线$A$的模型，学习算法$\\Im_b$基于另一种归纳偏好产生了对应于曲线$B$的模型。    \n",
    "基于前面讨论的平滑曲线的某种“描述简单性”，我们满怀信心地期待算法$\\Im_a$比$\\Im_b$更好。   \n",
    "确实，图1.4(a)显示出，与B相比，A与训练集外的样本更一致；\n",
    "换言之，A的泛化能力比B强。\n",
    "\n",
    "![title](../images/regression_02.png)\n",
    "\n",
    "\n",
    "但是，且慢！ \n",
    "虽然我们希望并相信$ \\Im_{a} $比$\\Im_{b}$更好，但会不会出现图1.4(b)的情况：    \n",
    "与A相比，B与训练集外的样本更一致？   \n",
    "\n",
    "很遗憾，这种情况完全可能出现。    \n",
    "换言之，对于一个学习算法$ \\Im_{a} $，若它在某些问题上比学习算法$\\Im_{b}$好，则必然存在另一些问题，在哪里$\\Im_{b}$比$\\Im_{a}$好。   \n",
    "\n",
    "有趣的是，这个结论对任何算法均成立，哪怕是把本书后面呢将要介绍的一些聪明算法作为$\\Im_a$，而将“随机胡猜”这样的笨拙算法作为$\\Im_b$。   \n",
    "惊讶吗？   \n",
    "让我们看看下面这个简短的讨论：   \n",
    "\n",
    "为了简单起见，假设样本空间$\\chi$和假设空间$H$都是离散的。   \n",
    "令$P(h|X,\\Im_a)$代表算法$\\Im_a$基于训练数据$X$产生假设$h$的概率，再令$f$代表我们希望学习的真实目标函数。\n",
    "$\\Im_a$的“训练集外误差”，即$\\Im_a$在训练集之外的所有样本上的误差为\n",
    "$$E_{ote}(\\Im_a|X,f) = \\sum_h \\sum_{x \\in {\\chi - X}} P(x) Sign( h(x) \\ne f(x)) P(h | X, \\Im_a)$$\n",
    "\n",
    "其中，Sign()是指示函数，若为真则取值1，否则取值为0。\n",
    "\n",
    "西瓜书上有二分类问题的推导，无踪差竟然与机器学习算法无关！    \n",
    "**对于任意两个学习算法$\\Im_a$和$\\Im_b$我们都有**\n",
    "$$\\sum_f E_{ote}(\\Im_a| X, f) = \\sum_f E_{ote}(\\Im_b | X,f)$$\n",
    "\n",
    "也就是说，无论学习算法$\\Im_a$多聪明、学习算法$\\Im_b$多笨拙，它们的期望性能竟然相同!\n",
    "这就是**“没有免费的午餐”定理（No Free Lunch Theorem，简称NFL定理）**。\n",
    "\n",
    "既然所有学习算法的期望性能都跟随机胡猜差不多，那机器学习有什么好学的？    \n",
    "\n",
    "我们需注意到，**NFL定理有一个重要前提：所有“问题”出现的机会相同、或所有问题同等重要。**   \n",
    "但实际情形并不是这样。   \n",
    "很多时候，我们只关注自己正在试图解决的问题（例如某个具体应用任务），希望为它找到一个解决方案，至于这个解决方案在别的问题、甚至在相似的问题上是否为好方案，我们并不关心。   \n",
    "\n",
    "> 例如，为了快速从A地达到B地，如果我们正在考虑的A地是南京鼓楼、B地是南京新街口，那么“骑自行车”是很好的解决方法；这个方案对A地是南京鼓楼、B地是北京新街口的情形显然很糟糕，但我们对此并不关心。\n",
    "\n",
    "事实上，上面NFL定理的简短论述过程中假设了$f$的均匀分布，而实际情形并非如此。  \n",
    "例如，回到我们熟悉的西瓜问题，考虑 { 假设1：好瓜 $\\leftrightarrow$ (色泽 = *）$\\bigwedge$ （根蒂=蜷缩）$\\bigwedge$ （敲声=浊响) }和 { 假设2： 好瓜 $\\leftrightarrow$ (色泽 = *）$\\bigwedge$ （根蒂=硬挺）$\\bigwedge$ （敲声=清脆) }。    \n",
    "从NFL定理可知，这两个假设同样好。   \n",
    "\n",
    "我们立即会想到符合条件的例子，对好瓜(色泽=青绿;娘蒂=蜷缩;敲声=浊响)是假设1 更好，而\n",
    "对好瓜(色泽=乌黑;根蒂=硬挺;敲声=清脆)则是假设2 更好.看上去的确是。    \n",
    "这样.然而需注意到， \" (根蒂=蜷缩;敲声=浊晌)\"的好瓜很常见，而\"(根蒂:硬挺;敲声z清脆)\"的好瓜罕见，甚至不存在。    \n",
    "\n",
    "所以，NFL定理更重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论“什么学习算法更好”毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好。   \n",
    "要谈论算法的相对优劣，必须要针对具体的学习问题；  \n",
    "在某些问题上表现好的学习算法，在另一些问题上却可能不尽如人意，学习算法自身的归纳偏好与问题是否相匹配，往往会起到决定性的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
