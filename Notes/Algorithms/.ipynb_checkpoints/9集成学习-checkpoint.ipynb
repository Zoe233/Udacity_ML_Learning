{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成学习 ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成学习（ensemble learning）可以说是现在非常火爆的机器学习方法了。  \n",
    "它本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。  \n",
    "也就是我们常说的“博采众长”。  \n",
    "\n",
    "集成学习可以用于\n",
    "- 分类问题集成，\n",
    "- 回归问题集成，\n",
    "- 特征选取集成，\n",
    "- 异常点检测集成等等，   \n",
    "可以说所有的机器学习领域都可以看到集成学习的身影。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:blue'> Part 1：集成学习框架</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.集成学习概述\n",
    "从下图，我们可以对集成学习的思想做一个概括。  \n",
    "对于训练数据集，我们通过若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。  \n",
    "\n",
    "![title](../images/ensemble_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是说，集成学习有两个主要问题需要解决：\n",
    "- 1. 如何得到若干个个体学习器\n",
    "- 2. 如何选择一种结合策略，将这些个体学习器集合成一个强学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.集成学习之个体学习器\n",
    "对于如何得到若干个个体学习器，我们有两种选择。\n",
    "- 1. 所有的个体学习器都是一个种类的，或者说同质的。   \n",
    "    - 比如都是决策树个体学习器，或者都是神经网络个体学习器。\n",
    "- 2. 所有的个体学习不全是一个种类的，或者说异质的。 \n",
    "    - 比如我们有一个分类问题，对训练集采用支持向量机个体学习器，逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过某种结合策略来确定最终的分类强学习器。\n",
    "    \n",
    "目前来说，同质个体学习器的应用是最广泛的，一般我们常说的集成学习的方法都是指的**同质个体学习器**。    \n",
    "而同质个体学习器使用最多的模型是**CART决策树**和**神经网络**。\n",
    "\n",
    "**同质个体学习器**按照个体学习器之间是否存在依赖关系可以分为两类：\n",
    "- 个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，代表算法是boosting系列算法；\n",
    "- 个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是bagging和随机森林（Random Forest）系列算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.集成学习之boosting\n",
    "boosting的算法原理我们可以用一张图做一个概括如下：\n",
    "![title](../images/ensemble_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图中可以看出：   \n",
    "Boosting算法的工作机制是\n",
    "1. 首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率$e_1$表现来更新训练样本的权重$\\alpha_1$，使得之前若学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。\n",
    "2. 然后基于调整权重后的训练集来训练若学习器2，如此重复进行，直到若学习器数达到事先指定的数目$T$，最终将这个$T$个弱学习器通过集合策略进行整合，得到最终的强学习器。\n",
    "\n",
    "\n",
    "Boosting系列算法里最著名算法主要有AdaBoost算法和提升树（boosting tree）系列算法。   \n",
    "提升树系列算法里面应用最广泛的是梯度提升树（Gradient Boosting Tree）。   \n",
    "AdaBoost和提升树算法的原理在后面会专门讲。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.集成学习之bagging\n",
    "Bagging的算法原理和boosting不同，它的弱学习器之间没有依赖关系，可以并行生成，我们可以用一张图做一个概括如下：\n",
    "![title](../images/ensemble_003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，bagging的个体弱学习器的训练集是通过随机采样得到的。   \n",
    "通过$T$次的随机采样，我们就可以得到$T$个采样集，对于这$T$个采样集，我们可以分别独立的训练出$T$个弱学习器，再对这$T$弱学习器通过集合策略来得到最终的强学习器。   \n",
    "\n",
    "对于这里的随机采用有必要做进一步的介绍，这里一般采用的是自助采样法（Boostap sampling），即对于m个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集，由于是随机采样，这样每次的采样集是和原始的训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。  \n",
    "\n",
    "随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。  \n",
    "所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，其基本思想没有脱离bagging的范畴。  \n",
    "bagging和随机森林算法的原理会在后面的文章中专门来讲。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.集成学习之结合策略\n",
    "假定我们得到的T个弱学习器是$\\{h_1, h_2,..., h_T\\}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 平均法\n",
    "对于数值类的回归预测问题，通常使用的集合策略是平均法，也就是说，对于若干个若学习器的输出进行平均得到最终的预测输出。  \n",
    "\n",
    "最简单的平均是算法平均，也就是说最终预测是  \n",
    "$$H(x) = \\frac{1}{T} \\sum_{i=1}^{T} h_i(x)$$\n",
    "\n",
    "如果每个个体学习器都有一个权重$w$，则最终预测是：\n",
    "$$H(x) = \\sum_{i=1}^T w_i h_i(x)$$\n",
    "其中，$w_i$是个体学习器$h_i$的权重，通常有\n",
    "$$ w_i \\geq 0, \\sum_{i=1}^T = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 投票法\n",
    "对于分类问题的预测，我们通常使用的是投票法。   \n",
    "假设我们的预测类别是$\\{ c_1, c_2, ... , c_K\\}$，对于任意一个预测样本$x$，我们的$T$个弱学习器的预测结果分别是$(h_1(x), h_2(x), h_3(x), ... , h_T(x))$。   \n",
    "\n",
    "最简单的投票法是相对多数投票法，也就是我们常说的少数服从多数，也就是$T$个弱学习器的对样本$x$的预测结果中，数量最多的类别$c_i$为最终的分类类别。  \n",
    "如果不止一个类别获得最高票，则随机选择一个做最终类别。  \n",
    "\n",
    "稍微复杂的投票法是**绝对多数投票法**，也就是我们常说的要票数过半。  \n",
    "在相对多数投票法的基础上，不光要求获得高票，还要求票过半数，否则会拒绝预测。   \n",
    "\n",
    "更加复杂的是**加权投票法**，和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别即为最终类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 学习法\n",
    "上两节的方法都是对弱学习器的结果做平均或者投票，相比比较简单，但是可能学习误差较大，于是就有了学习法这种方法。  \n",
    "\n",
    "对于学习法，代表方法是**stacking**，当使用stacking的结合策略时，我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。  \n",
    "\n",
    "在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。  \n",
    "\n",
    "对于测试集，我们首选用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:blue'> Part 2：Adaboost算法</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成学习按照个体学习器之间是否存在依赖关系可以分为两类：\n",
    "1. 个体学习器之间存在强依赖关系--代表算法：boosting系列算法\n",
    "2. 个体学习器之间不存在强依赖关系。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在boosting系列算法中，**Adaboost**是最著名的算法之一。  \n",
    "Adaboost既可以用作分类，也可以用作回归。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 回顾boosting算法的基本原理\n",
    "![title](../images/ensemble_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的**学习误差率**表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些**误差率高的点**在后面的**弱学习器2**中得到更多的重视。   \n",
    "然后基于调整权重后的训练集来训练弱学习器2，如此重复进行，直到弱学习器数达到实现指定的数目$T$，最终将这$T$个弱学习器通过集合策略进行整合，得到最终的强学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过有几个具体的问题Boosting算法没有详细说明：\n",
    "1. 如何计算学习误差率$e$？\n",
    "2. 如何得到弱学习权重系数$\\alpha$？\n",
    "3. 如何更新样本权重$D$？\n",
    "4. 使用何种结合策略？\n",
    "\n",
    "只要是boosting大家族的算法，都要解决这4个问题。  \n",
    "那么Adaboosting是怎么解决的呢?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
