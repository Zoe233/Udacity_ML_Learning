{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成学习 ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成学习（ensemble learning）可以说是现在非常火爆的机器学习方法了。  \n",
    "它本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。  \n",
    "也就是我们常说的“博采众长”。  \n",
    "\n",
    "集成学习可以用于\n",
    "- 分类问题集成，\n",
    "- 回归问题集成，\n",
    "- 特征选取集成，\n",
    "- 异常点检测集成等等，   \n",
    "可以说所有的机器学习领域都可以看到集成学习的身影。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.集成学习概述\n",
    "从下图，我们可以对集成学习的思想做一个概括。  \n",
    "对于训练数据集，我们通过若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。  \n",
    "\n",
    "![title](../images/ensemble_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是说，集成学习有两个主要问题需要解决：\n",
    "- 1. 如何得到若干个个体学习器\n",
    "- 2. 如何选择一种结合策略，将这些个体学习器集合成一个强学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.集成学习之个体学习器\n",
    "对于如何得到若干个个体学习器，我们有两种选择。\n",
    "- 1. 所有的个体学习器都是一个种类的，或者说同质的。   \n",
    "    - 比如都是决策树个体学习器，或者都是神经网络个体学习器。\n",
    "- 2. 所有的个体学习不全是一个种类的，或者说异质的。 \n",
    "    - 比如我们有一个分类问题，对训练集采用支持向量机个体学习器，逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过某种结合策略来确定最终的分类强学习器。\n",
    "    \n",
    "目前来说，同质个体学习器的应用是最广泛的，一般我们常说的集成学习的方法都是指的**同质个体学习器**。    \n",
    "而同质个体学习器使用最多的模型是**CART决策树**和**神经网络**。\n",
    "\n",
    "**同质个体学习器**按照个体学习器之间是否存在依赖关系可以分为两类：\n",
    "- 个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，代表算法是boosting系列算法；\n",
    "- 个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是bagging和随机森林（Random Forest）系列算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.集成学习之boosting\n",
    "boosting的算法原理我们可以用一张图做一个概括如下：\n",
    "![title](../images/ensemble_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图中可以看出：   \n",
    "Boosting算法的工作机制是\n",
    "1. 首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率$e_1$表现来更新训练样本的权重$\\alpha_1$，使得之前若学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。\n",
    "2. 然后基于调整权重后的训练集来训练若学习器2，如此重复进行，直到若学习器数达到事先指定的数目$T$，最终将这个$T$个弱学习器通过集合策略进行整合，得到最终的强学习器。\n",
    "\n",
    "\n",
    "Boosting系列算法里最著名算法主要有AdaBoost算法和提升树（boosting tree）系列算法。   \n",
    "提升树系列算法里面应用最广泛的是梯度提升树（Gradient Boosting Tree）。   \n",
    "AdaBoost和提升树算法的原理在后面会专门讲。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.集成学习之bagging\n",
    "Bagging的算法原理和boosting不同，它的弱学习器之间没有依赖关系，可以并行生成，我们可以用一张图做一个概括如下：\n",
    "![title](../images/ensemble_003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，bagging的个体弱学习器的训练集是通过随机采样得到的。   \n",
    "通过$T$次的随机采样，我们就可以得到$T$个采样集，对于这$T$个采样集，我们可以分别独立的训练出$T$个弱学习器，再对这$T$弱学习器通过集合策略来得到最终的强学习器。   \n",
    "\n",
    "对于这里的随机采用有必要做进一步的介绍，这里一般采用的是自助采样法（Boostap sampling），即对于m个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集，由于是随机采样，这样每次的采样集是和原始的训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。  \n",
    "\n",
    "随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。  \n",
    "所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，其基本思想没有脱离bagging的范畴。  \n",
    "bagging和随机森林算法的原理会在后面的文章中专门来讲。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
