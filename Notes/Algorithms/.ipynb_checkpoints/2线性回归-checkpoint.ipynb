{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回归Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Udacity内容\n",
    "## Regression:   \n",
    "- Supervised Learning: Take examples of inputs and outputs. Now, given a new input, predict its output. \n",
    "- Mapping continous inputs to outputs. (discrete, continous)\n",
    "\n",
    "## Regression & Function Approximation:\n",
    "例子child Height和函数逼近或回归有什么关系呢？\n",
    "向平均值回落的回归概念与逼近函数、输入到输出的映射有什么关系？\n",
    "\n",
    "例子Child Height是19世纪后期的研究成果，它将所有这些不同的量（父母的身高，人群的平均身高，目标子女的身高）巧妙地联系了起来。   \n",
    "即用这种函数的方式在它们之间创建了关系。  \n",
    "于是人们说“这真不错，我们要使用回归这个概念”（Regression to Mean）。    \n",
    "但他们开始使用后，真正的用意并非向均数回归，而是基于一堆测量点寻找一种数学关系，所以这个词在后来被误用了。  \n",
    "而我们现在依然在使用这个词。   \n",
    "所以，现在**回归并不是指向平均值回落，而是使用函数形式来逼近一堆数据点。**\n",
    "\n",
    "\n",
    "## Regression in Machine Learning:\n",
    "\n",
    "## Finding the best constant function:\n",
    "找到可以逼近所有点的最佳的常数。   \n",
    "**常数函数**：$f(x) = c$    \n",
    "**误差函数**：$E(c) = \\sum_{i=1}^{n}(y_{i} - c)^{2}$ ，LOSS ERROR.    \n",
    "想要求使得LOSS ERROR最小的常数项，对误差函数求导即可。   \n",
    "**求导计算过程**：\n",
    "    $\\frac{dE(c)}{dc} = \\sum_{i=1}^{n} 2*(y_{i} -c)*(-1)$     \n",
    "想要找到最小值，使得上式等于0即可。   \n",
    "    即$\\sum_{i=1}^{n} 2*(y_{i} -c)*(-1) = 0$    \n",
    "    $-\\sum_{i=1}^{n}2*(y_{i}-c) = 0$    \n",
    "    $\\sum_{i=1}^{n}y_{i} - \\sum_{i=1}^{n}c=0$   \n",
    "    $n*c = \\sum_{i=1}^{n}y_{i}$    \n",
    "    $c = \\frac{\\sum_{i=1}^{n}y_{i}}{c}$\n",
    "    \n",
    " 根据计算结果，最佳的常数是所有y的平均值。\n",
    " \n",
    " ## Order of Polynomial多项式的阶：\n",
    " k=0: constant    \n",
    " k=1: line     \n",
    " k=2: parabola    \n",
    "     $f(x) = c_{0} + c_{1}*x + c_{2}*x^{2} + ... + c_{k}*x^{k}$  \n",
    " \n",
    " ![title](../images/regression_1.png)\n",
    " \n",
    " 在这个例子中，考虑到我们所有的数据量（n=9），我们不能超过的数据点的个数，大于n的都不行，对它们的约束不够。  \n",
    " 在这个例子中，最大的order是8.\n",
    " \n",
    " ## Polynomial Regression 多项式回归：\n",
    " ![title](../images/polynomial.png)\n",
    " \n",
    " 上图用矩阵形式表达为：\n",
    " $Xw\\thickapprox{y}$    \n",
    " 我们基本上只需要对$w$这个向量求解，现在我们无法真正对它求解，因为上面这个式子并不是完全相等的，但是我们可以用最小二乘法思想来求解。  \n",
    " 下面逐步完成所需的每个步骤：   \n",
    " ![tilte](../images/w_1.png)\n",
    " \n",
    " 正好这些x转置和x有一些不错的属性，$(X^{T}X)^{-1}$不仅可逆，它还可以最小化最小二乘。  它的作用是射影，现在我们将不对它的正确性进行证明。\n",
    " \n",
    " ## Errors:\n",
    " 我们无法通过解线性方程组来解答这类问题的部分原因，\n",
    " 我们不得不使用这些平方计算的原因是**存在误差**，就是提供给我们的训练数据存在误差。\n",
    " 并不是我们要真正建模的不是函数，而我们见到的函数加上每条数据上的误差项。\n",
    " \n",
    " Training data has errors.    \n",
    " Not modeling f, but fits.    \n",
    " \n",
    " \n",
    " 我认为有理由想一想这些误差是如何产生的。  \n",
    " 为什么我们要试图拟合有误差的数据？\n",
    " \n",
    " Where do errors come from?\n",
    " - sensor error\n",
    " - maliciously - being given bad data\n",
    " - transcription error\n",
    " - unmodeled influences\n",
    " \n",
    " \n",
    " ## Cross Validation:\n",
    " 我们的目的始终是泛化generalize。\n",
    " \n",
    " ## Conclusion：\n",
    " What did we learn?\n",
    " - historial facts\n",
    " - model selection and under/overfitting\n",
    " - cross validation\n",
    " - linear, Polynomial regression\n",
    " - best constant in terms of squared error is mean\n",
    " - representation for regression\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn\n",
    "sklearn.linear_model.LinearRegression\n",
    "\n",
    "官方示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope:  [0.5 0.5]\n",
      "intercept:  2.220446049250313e-16\n",
      "predicted:  [3.]\n",
      "r-squared score:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "\n",
    "print('slope: ', reg.coef_)\n",
    "print('intercept: ', reg.intercept_)\n",
    "print('predicted: ', reg.predict([[3, 3]]))\n",
    "print('r-squared score: ', reg.score([[4, 4], [5, 5]],  [4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a href='../../Projects/Project_5_Regressions/linear_regression_main.ipynb' style='color:red'>sklearn简单线性回归实例</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Errors 线性回归误差:\n",
    "### 如何评估线性回归？  \n",
    "- 你可以执行的一项明显操作是可视化，将你的回归结果放在散点图上。  \n",
    "- 另一项操作是查看线性回归产生的误差。\n",
    "    error = actual value - predicted value\n",
    "![title](../images/error_1.png)\n",
    "\n",
    "\n",
    "### 拟合良好的合适标准是什么？拟合能将什么情况下的误差降至最低呢？\n",
    "执行线性回归时，你的工作是最大程度地降低误差平方和，这意味着，最佳回归时能最大程度地降低误差平方和（Sum of Squared Due to Error, SSE）的回归。    \n",
    "\n",
    "最受欢迎的两种算法：\n",
    "- **普通最小二乘法Ordinary Least Squares（OLS）**。\n",
    "    - used in sklearn LinearRegression\n",
    "- **梯度下降Gradient Descent**\n",
    "\n",
    "### 为什么使用误差平方和SSE来最小化获得最佳拟合的回归线呢？\n",
    "因为使用误差绝对值时，回归所在的准确范围方面存在固有的模糊性，它可以在这一范围内的任何位置。换句话说，可以有多条能最大程度降低绝对误差的线，但只有一条可以最大程度降低误差平方的线。   \n",
    "Using SSE also makes implementation much easier.  \n",
    "\n",
    "\n",
    "### 误差平方和SSE评估回归时的缺点：\n",
    "误差平方和受数据量的影响大，添加的数据越多，误差平方和几乎必定增加。   \n",
    "但是这并不表示回归线拟合的就不好。   \n",
    "\n",
    "### 针对误差平方和SSE的问题（受数据量影响）-- $R^{2}$\n",
    "决定系数$R^{2}$是一种非常受欢迎的评估指标，可描述线性回归的拟合良好度。   \n",
    "$R^{2}$是一个可以有效回答下面问题的数字：\n",
    "- how much of my change in the output(y) is explained by the change in my input(x) ？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^{2}$的数值范围从0至1，表示目标变量的预测值和实际值之间的相关程度平方的百分比。   \n",
    "一个模型的$R^{2}$值为0还并不如直接用平均值来预测效果好；而一个$R^{2}$值为1的模型则可以对目标变量进行完美的预测。   \n",
    "从0至1之间的数值，则表示该模型中目标变量中有百分之多少能够用特征来解释。  \n",
    "模型也可能出现负值的$R^{2}$，这种情况下模型所做预测有时会比直接计算目标变量的平均值差很多。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在对数据进行线性回归计算之后，我们能够得到相应函数的系数，那么我们如何知道得出的这个系数对方程结果的影响有多强呢？\n",
    "\n",
    "所以我们用到了一种方法叫做 Coefficient of Determination （决定系数）来判断**回归方程**拟合的程度。\n",
    "\n",
    "**前提概念：**\n",
    "1. Sum of Squared Due to Error  误差平方和   \n",
    "    $SSE = \\sum{(y_{i} - \\hat{y}_{i})}$\n",
    "   <br>第i个观察点，真实数据$y_{i}$与估计值$\\hat{y}_{i}$之间的差被称为第i个residual(残差），SSE就是所有观察点的residual的和.</br>\n",
    "   \n",
    "   \n",
    "2. Total Sum of Squares   总的平方和   \n",
    "    $SST = \\sum{(y_{i} - \\bar{y})^{2}}$\n",
    "        \n",
    "        \n",
    "3. Sum of Squares Due to Regression   回归平方和   \n",
    "    $SSR = \\sum{(\\hat{y}_{i} - \\bar{y})^{2}}$\n",
    "    \n",
    "    \n",
    "通过以上我们能得到以下关于他们三者的关系：  \n",
    "    $SST = SSR + SSE$\n",
    "\n",
    "\n",
    "**决定系数：判断 回归方程 的拟合程度**  \n",
    "通过回归方程得出的dependent variable(依赖变量）有numer% 能被independent variable所揭示。  \n",
    "$R^{2}$决定系数（拟合优度）：  \n",
    "$r^{2} = \\frac{SSR}{SST}$    \n",
    "$R^{2}(y,\\hat{y}) = 1 - \\frac{\\sum(y_{i} -\\hat{y}_{i})^{2}}{\\sum(y_{i} -\\bar{y}_{i})^{2}}$ \n",
    "<br>$R^{2}(y,\\hat{y}) = 1 - \\frac{SSE}{SST}$</br>\n",
    "\n",
    "**相关系数Correlation Coefficient：测试dependent variable和independent variable他他们之间的线性关系有多强**。  \n",
    "也就是说，independent variable产生变化时dependent variable的变化有多大。  \n",
    "可以反应是正相关还是负相关。  \n",
    "\n",
    "** Sample correlation coefficient **:  \n",
    "    $r_{xy} $= (sign of  $b_{1})\\sqrt{Coefficient Of Determination}$   \n",
    "               =(sign of $b_{1}$)$\\sqrt{r^{2}}$  \n",
    "where $b_{1}$ = the slope of the estimated regression equation $\\hat{y} = b_{0} + b_{1}x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类和回归的比较Comparing Classfication & Regression\n",
    "| property | supervised classification | regression |\n",
    "|:--:|:--:|:--:|\n",
    "| output type | discrete (class labels) | continuous (number) |\n",
    "| what are you trying to find? | decision boundary | \"best fit line\"|\n",
    "| evaluation | accuracy | \"Sum of Squared Error\" or $r^{2}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Name|中文名|\n",
    "|:-:|:-|\n",
    "|Simple Linear Regression| 简单线性回归|\n",
    "|Multi-Variate Regression| 多元线性回归|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整理\n",
    "<a href='http://www.cnblogs.com/pinard/p/6004041.html'>参考页面</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.线性回归的模型函数和损失函数\n",
    "线性回归遇到的问题一般都是这样的。  \n",
    "我们有m个样本，每个样本对应于n维特征和一个结果输出，如下：  \n",
    "$(x_{1}^{(0)}, x_{2}^{(0)}, ... , x_{n}^{(0)}, y_{0}),  (x_{1}^{(1)}, x_{2}^{(1)}, ... , x_{n}^{(1)}, ... ,  y_{1}),(x_{1}^{(m)}, x_{2}^{(m)}, ... , x_{n}^{(m)}, y_{n})$   \n",
    "我们的问题是，对于一个新的$(x_{1}^{(x)}, x_{2}^{(x)}, ... , x_{n}^{(x)})$，它所对应的$y_{x}$是多少呢？   \n",
    "- 如果这个问题里面的$y$是连续的，则是一个回归问题，否则是一个分类问题。   \n",
    "\n",
    "对于n维特征的样本数据，如果我们决定使用线性回归，那么对应的模型是这样的：\n",
    "$h_{\\theta}(x_{1}, x_{2}, ... , x_{n}) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{n}x_{n}$，   \n",
    "其中$\\theta_{i}(i = 0,1,2,3,...,n)为模型参数;     \n",
    "$x_{i}(i = 0,1,2,...,n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
