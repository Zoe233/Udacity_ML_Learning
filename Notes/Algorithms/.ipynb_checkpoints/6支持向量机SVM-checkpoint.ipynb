{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机 Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量机虽然诞生只有短短的二十多年，但是自一诞生便由于它良好的分类性能席卷了机器学习领域，并牢牢压制了神经网络领域好多年。   \n",
    "如果不考虑集成学习的算法，不考虑特定的训练数据集，在分类算法中表现SVM说是排第一估计是没有什么异议的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM是一个二元分类算法，线性分类和非线性分类都支持。  \n",
    "经过演化，现在也可以支持多元分类，同时经过拓展，也能应用于回归问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.回顾感知机模型\n",
    "感知机模型就是尝试找到一条直线，能够把二元数据隔离开。   \n",
    "放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。   \n",
    "对于这个分离的超平面，我们定义为$$w^T x + b = 0$$，如下图。\n",
    "![title](../images/perceptron_model1.png)\n",
    "\n",
    "在超平面$w^T x + b = 0$的上方我们定义$y=1$，在超平面$w^T x + b = 0$的下方，我们定义$y=-1$。    \n",
    "可以看出满足这个条件的超平面并不止一个。  \n",
    "那么我们可能会思考，这么多的可以分类的超平面，哪个是最好的呢？   \n",
    "或者说哪个的泛化能力是最强的呢？   \n",
    "\n",
    "\n",
    "接着我们看感知机模型的损失函数优化，它的思想是让所有误分类的点(定义为M)到超平面的距离和最小，即最小化下式：\n",
    "$$\\sum_{x_i \\in M} -y^{(i)} (w^T x^{(i)} + b) // ||w||_2$$\n",
    "\n",
    "当$w$和$b$成比例的增加，比如，当分子的$w$和$b$拓大$N$倍时，分母的$L2$范数也会扩大$N$倍。  \n",
    "也就是说，分子和分母有固定的倍数关系。   \n",
    "那么我们可以固定分子或者分母为1，然后求另一个即分子自己或者分母的导数的最小化作为损失函数，这样可以简化我们的损失函数。   \n",
    "\n",
    "在感知机模型中，我们采用的是保留分子，固定分母$||w||_2 = 1$，即最终感知机模型的损失函数为：\n",
    "$$\\sum_{x_i \\in M} -y^{(i)} (w^T x^{(i)} + b )$$\n",
    "\n",
    "如果我们不是固定分母，改为固定分子，作为分类模型有没有改进呢？   \n",
    "这些问题在我们因纳入SVM后会详细解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.函数间隔与几何间隔\n",
    "在正式介绍SVM的模型和损失函数之前，我们还需要先了解下函数间隔和几何间隔的知识。\n",
    "\n",
    "在分离超平面固定为$w^T x + b =0 $的时候，$|w^T x + b|$表示点$x$到超平面的距离。   \n",
    "通过观察$w^T x + b$与$y$是否同号，我们判断分类是否正确，这些知识我们在感知机模型里都有讲到。   \n",
    "\n",
    "这里我们引入函数间隔的概念，定义函数间隔$\\gamma'$为：\n",
    "$$\\gamma' = y(w^T + b)$$\n",
    "\n",
    "可以看到，它就是感知机模型里面的误分类点到超平面距离的分子。  \n",
    "对于训练集中m个样本点对应的m个函数间隔的最小值，就是整个训练集的函数间隔。   \n",
    "\n",
    "函数间隔并不能正常反应点到超平面的距离，在感知机模型里我们也提到，当分子成比例的增长时，分母也是成倍增长。   \n",
    "为了统一度量，我们需要对法向量$w$加上约束条件，这样我们就得到了几何间隔$\\gamma$，定义为：\n",
    "$$\\gamma = \\frac{y(w^T x + b)}{||w||_2} = \\frac{\\gamma '}{||w||_2}$$\n",
    "\n",
    "几何间隔才是点到超平面的真正距离，感知机模型里用到的距离就是几何距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 支持向量\n",
    "在感知机模型中，我们可以找到多个可以分类的超平面将数据分开，并且优化时希望所有点都离超平面远。   \n",
    "但是实际上离超平面很远的点已经被正确分类，我们让它离超平面更远并没有意义。   \n",
    "反而我们最关心是哪些离超平面很近的点，这些点很容易被误分类。   \n",
    "如果我们可以让离超平面比较近的点尽可能的远离超平面，那么我们的分类效果会好一些。     \n",
    "SVM的思想正起于此。   \n",
    "\n",
    "如下图所示，分离超平面为$w^T x + b = 0$，如果所有的样本不光可以被超平面分开，还和超平面保持一定的函数距离（下图函数距离为1），那么这样的超平面是比感知机的分类超平面优的。   \n",
    "并且可以证明，这样的超平面只有一个。   \n",
    "和超平面平行的保持一定的函数距离的这两个超平面对应的向量，我们定义为支持向量，如下图虚线所示。   \n",
    "![title](../images/perceptron_distance.png)\n",
    "\n",
    "\n",
    "支持向量到超平面的距离为$\\frac{1}{||w||_2}$，两个支持向量之间的距离为$\\frac{2}{||w||_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
